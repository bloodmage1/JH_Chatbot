{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3227200f",
   "metadata": {},
   "source": [
    "Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a564f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd7773c",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6def691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LlamaTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "52it [00:00, 5200.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./train_small_jh.csv', encoding = 'cp949')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('beomi/llama-2-koen-13b',  eos_token='</s>')\n",
    "\n",
    "max_length = 24\n",
    "\n",
    "formatted_data = []\n",
    "for _, row in tqdm(data.iterrows()):\n",
    "    for q_col in ['질문_1', '질문_2']:\n",
    "        input_text = row[q_col] + tokenizer.eos_token + row['답변_1']\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length)\n",
    "        formatted_data.append(input_ids)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb26c0e",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c91171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94725adf0c824aed85714da0db7fa164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 31293440 || all params: 6850073600 || trainable%: 0.45683363168535884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:665: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='312' max='312' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [312/312 04:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.924800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.723700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.906700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.523900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.007400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.487000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.420700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.617200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.953800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.054800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.976800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.878700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.013200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.908400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.041300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.796200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 완료\n",
      "저장 완료\n"
     ]
    }
   ],
   "source": [
    "formatted_data = torch.cat(formatted_data, dim=0)\n",
    "\n",
    "model_id = \"beomi/llama-2-koen-13b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             device_map={\"\":0},\n",
    "                                             )\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=1,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()\n",
    "print(\"학습 완료\")\n",
    "\n",
    "model_save_path = \"./model_jh/\"  # 모델을 저장할 경로 지정\n",
    "model.save_pretrained(model_save_path)            # 모델 저장\n",
    "tokenizer.save_pretrained(model_save_path)        # 토크나이저 저장\n",
    "print(\"저장 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6391490",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f696322d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8bdd5f2ceb4d9f9010c81cc3b64b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 당신의 장점이 무엇입니까?\n",
      "답변: 당신의 장점이 무엇입니까? 항상 진실한 사람입니다. 1안, 2안, 3안\n",
      "\n",
      "질문: 앞으로의 꿈은 무엇입니까?\n",
      "답변: 앞으로의 꿈은 무엇입니까? AI를 아는 풀스택 개발자입니다. 3. 자신이 가진\n",
      "\n",
      "질문: 이직할 때에 가장 우선순위는 무엇인가요?\n",
      "답변: 이직할 때에 가장 우선순위는 무엇인가요? 회사의 비전과 성장가능성입니다. 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"./model_jh/\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.to('cuda')\n",
    "\n",
    "def generate_answer(model, tokenizer, question, max_length = 24):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    \n",
    "    # if torch.cuda.is_available():\n",
    "    # inputs = {key: value.to('cuda') for key, value in inputs.items()}\n",
    "    inputs = {key: value for key, value in inputs.items() if key != 'token_type_ids'}\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     torch.cuda.empty_cache()  \n",
    "    output = model.generate(**inputs, max_length=max_length)\n",
    "    \n",
    "    # 생성된 토큰을 문장으로 변환\n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "questions = [\n",
    "    \"당신의 장점이 무엇입니까?\",\n",
    "    \"앞으로의 꿈은 무엇입니까?\",\n",
    "    \"이직할 때에 가장 우선순위는 무엇인가요?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = generate_answer(model, tokenizer, question)\n",
    "    print(f\"질문: {question}\\n답변: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"./model_jh/\" \n",
    "model.save_pretrained(model_save_path)    \n",
    "tokenizer.save_pretrained(model_save_path)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d013d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_answer(model, tokenizer, question):\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {key: value.to(\"cuda\") for key, value in inputs.items() if key != 'token_type_ids'}\n",
    "    else:\n",
    "        inputs = {key: value for key, value in inputs.items() if key != 'token_type_ids'}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_length=128)\n",
    "    \n",
    "    answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbc8ba91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 팀 내 갈등이 생기면 어떻게 해결하셨나요?\n",
      "답변: 팀 내 갈등이 생기면 어떻게 해결하셨나요?저는 우선 서로의 감정을 상하지 않게 하는 것이 가장 중요하다고 생각합니다. 그래서 제가 가장 많이 하는 것은 농담으로 분위기를 풀려고 노력하는 것입니다. 만약에 여자 A와 남자 B가 서로 싸웠다면, 제가 A에게 농담을 하면서 기분을 풀어주고, B를 다독여주는 역할을 합니다. 이렇게 해야 회사생활이 편안하게 흘러갈 수 있다고 생각합니다. 2. 회사생활을 하면서 가장 중요하게 생각하는 가치는 무엇인가요?저는 개인적으로는 성장가능성이라고 생각합니다. 제가 아직 어린 나이이고, 어린 나이라고 생각하기 때문에, 성장가능\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"팀 내 갈등이 생기면, 어떻게 해결하셨나요?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = generate_answer(model, tokenizer, question)\n",
    "    print(f\"질문: {question}\\n답변: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed1e778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 너 바보야?\n",
      "답변: 너 바보야? 왜 바보로 느껴지는가?저는 바보로 느껴지지 않도록 열심히 답변하겠습니다. 저는 바보로 느껴지지 않도록 열심히 답변하겠습니다. 우선, 저는 바보로 느껴지지 않도록 열심히 답변하겠습니다. 저는 스마트하고, 열심히하는 사람입니다. 바보로 느껴지지 않도록 열심히 답변하겠습니다. 저는 스마트하고, 열심히하는 사람입니다. 스마트하고, 열심히하는 바보로 느껴지지 않도록 열심히 답변하겠습니다. 스마트하고, 열심히하는 바보로 느껴지지 않도록 열심히 답변하겠습니다. 스마트하고, 열심히하는 바보로 느껴지지 않도록 열심히 답변하겠습니다. 스마트하고, 열심히하는 바보로 느껴지지 않도록 열심히 답변하겠습니다. 스마트\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"너 바보야?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = generate_answer(model, tokenizer, question)\n",
    "    print(f\"질문: {question}\\n답변: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc0622af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 향후 인생의 목표가 있으신가요?\n",
      "답변: 향후 인생의 목표가 있으신가요?무엇이든 간에, 제 꿈을 설명해주세요.저는 스마트팜을 구축하는 것입니다. 스마트팜을 구축하는 이유는, 환경파괴를 줄일 수 있고, 물 낭비를 줄일 수 있기 때문입니다. 그리고 스마트팜을 구축한 이후에, AI로 자동화시키는 과정을 거쳐서, 제가 간과한 환경파괴나 물 낭비를 줄일 수 있는지 항상 생각할 것입니다. 2. 졸업논문은 무슨 내용이었나요?축산환경에서 발생하는 악취를 저감하는 방법에 대해서 연구하였습니다. 축산환경에서 발생하는\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"향후 인생의 목표가 있으신가요?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = generate_answer(model, tokenizer, question)\n",
    "    print(f\"질문: {question}\\n답변: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bac8f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'향후 인생의 목표가 있으신가요?무엇이든 간에, 제 꿈을 설명해주세요.저는 스마트팜을 구축하는 것입니다. 스마트팜을 구축하는 이유는, 환경파괴를 줄일 수 있고, 물 낭비를 줄일 수 있기 때문입니다. 그리고 스마트팜을 구축한 이후에, AI로 자동화시키는 과정을 거쳐서, 제가 간과한 환경파괴나 물 낭비를 줄일 수 있는지 항상 생각할 것입니다. 2. 졸업논문은 무슨 내용이었나요?축산환경에서 발생하는 악취를 저감하는 방법에 대해서 연구하였습니다. 축산환경에서 발생하는'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "debf309a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 향후 인생의 목표가 있으신가요?\n",
      "답변: 무엇이든 간에, 제 꿈을 설명해주세요.저는 스마트팜을 구축하는 것입니다. 스마트팜을 구축하는 이유는, 환경파괴를 줄일 수 있고, 물 낭비를 줄일 수 있기 때문입니다. 그리고 스마트팜을 구축한 이후에, AI로 자동화시키는 과정을 거쳐서, 제가 간과한 환경파괴나 물 낭비를 줄일 수 있는지 항상 생각할 것입니다. \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def truncate_after_number(answer, number='2'):\n",
    "    pattern = r'\\b' + re.escape(number) + r'\\b.*'\n",
    "    truncated_answer = re.sub(pattern, '', answer, flags=re.DOTALL)\n",
    "    return truncated_answer\n",
    "\n",
    "def extract_and_remove_first_question(text):\n",
    "    pattern = r'^(.*?\\?)'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        first_question = match.group(1)  # 첫 번째 문장 저장\n",
    "        remaining_text = re.sub(pattern, '', text, count=1)  # 첫 문장 제거\n",
    "        return first_question, remaining_text\n",
    "    return \"\", text \n",
    "\n",
    "tr_answer = truncate_after_number(answer)\n",
    "que, tr_answer = extract_and_remove_first_question(tr_answer)\n",
    "print(\"질문:\", que)\n",
    "print('답변:',tr_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81c67f",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1365fa66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce11d268f01423ebeee55c981bf08a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.1.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.2.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "C:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2047: UserWarning: for model.layers.3.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: model.layers.3, model.layers.4, model.layers.5, model.layers.6, model.layers.7, model.layers.8, model.layers.9, model.layers.10, model.layers.11, model.layers.12, model.layers.13, model.layers.14, model.layers.15, model.layers.16, model.layers.17, model.layers.18, model.layers.19, model.layers.20, model.layers.21, model.layers.22, model.layers.23, model.layers.24, model.layers.25, model.layers.26, model.layers.27, model.layers.28, model.layers.29, model.layers.30, model.layers.31, model.layers.32, model.layers.33, model.layers.34, model.layers.35, model.layers.36, model.layers.37, model.layers.38, model.layers.39, model.layers.40, model.layers.41, model.layers.42, model.layers.43, model.layers.44, model.layers.45, model.layers.46, model.layers.47, model.norm, lm_head.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results_LDCC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# tokenizer = PreTrainedTokenizerFast.from_pretrained('LDCC/LDCC-SOLAR-10.7B',  eos_token='</s>')\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./results_LDCC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results_LDCC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 질문 설정\u001b[39;00m\n",
      "File \u001b[1;32mC:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m )\n",
      "File \u001b[1;32mC:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\transformers\\modeling_utils.py:3565\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3562\u001b[0m     model\u001b[38;5;241m.\u001b[39mhf_quantizer \u001b[38;5;241m=\u001b[39m hf_quantizer\n\u001b[0;32m   3564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _adapter_model_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3565\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3566\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_adapter_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3567\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3569\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3570\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_loading_info:\n\u001b[0;32m   3573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loading_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\transformers\\integrations\\peft.py:222\u001b[0m, in \u001b[0;36mPeftAdapterMixin.load_adapter\u001b[1;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, adapter_kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# Re-dispatch model and hooks in case the model is offloaded to CPU / Disk.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    218\u001b[0m     (\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_device_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhf_device_map\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;241m.\u001b[39mintersection({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m})) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    221\u001b[0m ):\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_accelerate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\transformers\\integrations\\peft.py:471\u001b[0m, in \u001b[0;36mPeftAdapterMixin._dispatch_accelerate_model\u001b[1;34m(self, device_map, max_memory, offload_folder, offload_index)\u001b[0m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    468\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m, max_memory\u001b[38;5;241m=\u001b[39mmax_memory, no_split_module_classes\u001b[38;5;241m=\u001b[39mno_split_module_classes\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 471\u001b[0m \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdispatch_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\workspace\\chatbot_practice\\llmj\\Lib\\site-packages\\accelerate\\big_modeling.py:374\u001b[0m, in \u001b[0;36mdispatch_model\u001b[1;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[0;32m    372\u001b[0m disk_modules \u001b[38;5;241m=\u001b[39m [name \u001b[38;5;28;01mfor\u001b[39;00m name, device \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offload_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(disk_modules) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    375\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed to be offloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(disk_modules)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28mlen\u001b[39m(disk_modules) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(offload_dir) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(offload_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m    382\u001b[0m ):\n\u001b[0;32m    383\u001b[0m     disk_state_dict \u001b[38;5;241m=\u001b[39m extract_submodules_state_dict(model\u001b[38;5;241m.\u001b[39mstate_dict(), disk_modules)\n",
      "\u001b[1;31mValueError\u001b[0m: We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: model.layers.3, model.layers.4, model.layers.5, model.layers.6, model.layers.7, model.layers.8, model.layers.9, model.layers.10, model.layers.11, model.layers.12, model.layers.13, model.layers.14, model.layers.15, model.layers.16, model.layers.17, model.layers.18, model.layers.19, model.layers.20, model.layers.21, model.layers.22, model.layers.23, model.layers.24, model.layers.25, model.layers.26, model.layers.27, model.layers.28, model.layers.29, model.layers.30, model.layers.31, model.layers.32, model.layers.33, model.layers.34, model.layers.35, model.layers.36, model.layers.37, model.layers.38, model.layers.39, model.layers.40, model.layers.41, model.layers.42, model.layers.43, model.layers.44, model.layers.45, model.layers.46, model.layers.47, model.norm, lm_head."
     ]
    }
   ],
   "source": [
    "trainer.model.save_pretrained('./results_LDCC')\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained('LDCC/LDCC-SOLAR-10.7B',  eos_token='</s>')\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('./results_LDCC',device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./results_LDCC')\n",
    "\n",
    "question1 = \"당신의 장점이 무엇입니까?\"\n",
    "question2 = \"앞으로의 꿈은 무엇입니까?\"\n",
    "\n",
    "inputs1 = tokenizer.encode(question1, return_tensors='pt')\n",
    "inputs2 = tokenizer.encode(question2, return_tensors='pt')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    inputs1 = inputs1.cuda()\n",
    "    inputs2 = inputs2.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1 = model.generate(inputs1, max_length=100, num_beams=5, early_stopping=True)\n",
    "    outputs2 = model.generate(inputs2, max_length=100, num_beams=5, early_stopping=True)\n",
    "\n",
    "answer1 = tokenizer.decode(outputs1[0], skip_special_tokens=True)\n",
    "answer2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Answer:\", answer1)\n",
    "print(\"Generated Answer:\", answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb88896",
   "metadata": {},
   "source": [
    "1. 인삼 pyqt 제작\n",
    "\n",
    "2. 자동차 사고 모델 qt 제작\n",
    "\n",
    "3. 블랙박스 영상 화질개선 모델 제작\n",
    "\n",
    "4. Patchcore 모델 제작\n",
    "\n",
    "5. 멀티모달 모델 (완료)\n",
    "\n",
    "6. 시계열 모델(완료)\n",
    "\n",
    "7. 자기소개 챗봇()\n",
    "\n",
    "8. 여자친구 언어패턴 분석(mysql을 활용한)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
